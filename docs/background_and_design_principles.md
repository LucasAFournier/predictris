## Background and Design Principles

### Design Principles

We articulate three principles guiding the model.

* **P1 — Prediction as Intrinsic Motivation**: Without any external reward, the agent is driven by curiosity—maximising predictive accuracy—forcing it to build a comprehensive, task‑agnostic understanding of its environment.
* **P2 — Sequence‑based Representation for Sensorimotricity**: The world model captures sensorimotor traces of the form “if I observe $o$ and execute $a$, I expect $o'$”, explicitly disambiguating situations and making the model highly explainable. We focus on the deterministic case as a proof‑of‑concept.
* **P3 — Online Learning for Adaptability**: The model is refined at every time‑step through interaction, making the learning process active rather than passive.

### Sensorimotor Contingencies and Active Perception

Early theories of perception emphasize that sensation is fundamentally *active* and tied to motor actions. In the ecological psychology view, perception exploits **sensorimotor contingencies**, i.e. systematic relationships between an agent’s actions and the resulting sensory changes [@Gibson1979]. Rather than passively receiving stimuli, an agent learns the *rules* of interaction: for example, turning one’s head changes the retinal image in a lawful way. O’Regan and Noë’s seminal sensorimotor contingency theory formalized how mastery of these action–perception regularities underpins robust perception [@ORegan2001]. This enactive perspective, building on Gibson’s notion of **affordances**, has influenced both cognitive science and robotics. It suggests that cognition emerges from active sensorimotor loops, motivating artificial systems that *learn by doing* rather than by static observation. Recent computational frameworks like predictive processing align with this view, proposing that perception, action, and prediction form a coupled loop in which agents actively generate sensory predictions and minimize surprises [@Clark2013]. Overall, treating perception as an action-driven process grounds learning in the agent’s own experience, a principle central to our sensorimotor sequence learning approach.  

### Sequence Learning in Embodied Agents

**Sequence learning** is crucial for embodied agents that must anticipate the consequences of actions in a dynamic world. Unlike static pattern recognition, an embodied agent continually produces and observes sequences of state transitions. Classic work in artificial cognitive systems has aimed to learn *sensorimotor schemata* or *world models* from such experience. For example, Cohen *et al.* (1997) introduced the **Neural EM** (NEO) system that learned conceptual knowledge by interacting with an environment, forming if-then predictive rules from its sensorimotor sequences [@Cohen1997]. In reinforcement learning and robotics, **Predictive State Representations (PSRs)** were proposed as a way to model the agent’s state *only* through predictions of future observation-action sequences [@Singh2012]. A PSR represents state as a vector of expected outcomes for various action sequences (tests), rather than latent symbols, thus capturing the history in terms of what will happen next. This idea of representing history by *predictive* features has been influential in partially observable environments. Concurrently, recurrent neural networks have been applied to encode sensorimotor sequences in embodied agents. For instance, Ito and Tani (2004) demonstrated that a humanoid robot controlled by an RNN with predictive coding could learn to **imitate multiple movement patterns** observed from a human [@Ito2004]. Such models learn internal sequence representations that allow the agent to recognize and reproduce complex action trajectories. Overall, whether using symbolic structures or neural networks, learning to predict *temporal sequences* of observations is a foundational capability for agents to understand and navigate their world.

### Intrinsic Motivation and Prediction-Based Learning

A key challenge in online learning is how to drive the agent to explore and improve without external rewards. **Intrinsic motivation** addresses this by using the agent’s own predictive success or errors as an internal reward signal. Schmidhuber’s early work on artificial curiosity argued that agents should seek novel, learnable patterns – essentially maximizing the improvement in prediction or compression [@Schmidhuber1991]. In our context, **prediction itself acts as the reward**: the agent is “curious” to reduce prediction errors. This principle (P1) has been adopted in numerous frameworks, where an agent that accurately predicts its sensorimotor outcomes finds that experience intrinsically rewarding [@ORegan2001]. Modern reinforcement learning research has formalized curiosity as the error of a learned forward model. Pathak *et al.* (2017) implemented an **Intrinsic Curiosity Module (ICM)** that gives the agent a reward equal to its prediction error for the consequence of its actions [@Pathak2017]. This encourages exploration even in the absence of any extrinsic feedback: the agent learns for the *sake of learning*. Similarly, other intrinsic motivation approaches use surprise, uncertainty, or learning progress as signals to drive behavior. Without any external task, an intrinsically motivated agent can *autonomously acquire a broad repertoire of predictions*. This ensures a comprehensive, task-agnostic understanding of the environment [@ORegan2001].

### Adaptive Memory Structures for Prediction: Prediction Trees and Forests

Storing and updating a large number of sensorimotor sequences online requires an **adaptive memory structure** that can grow with experience. One inspiration comes from *variable-order Markov models* and suffix-tree representations. A **Prediction Suffix Tree (PST)** explicitly maintains a tree of histories (sequence suffixes) that are sufficient for predicting the next observation [@Ron1996]. Unlike a fixed-order Markov chain, a PST adapts the depth of context: certain branches extend longer if needed to disambiguate predictions, while others remain shallow. This yields a compact model that captures high-order dependencies only where necessary, achieving better prediction accuracy with lower memory than a full-order Markov model [@Ron1996]. Our proposed **Prediction Paths** and **Prediction Trees** follow a similar spirit, incrementally extending a sequence only when the current prefix becomes unreliable for prediction [@Fabbri2016] [@Perotto2013]. This approach is reminiscent of **constructivist learning algorithms** that build structure on the fly. For example, Fabbri *et al.* (2016) introduced a self-acquiring knowledge process for Monte Carlo Tree Search, wherein a **Background History Reply Forest (BHRF)** was used to **memorize efficient patterns** from gameplay experience [@Fabbri2016]. Their system, inspired by constructivist principles, accumulates a forest of frequently useful action-state sequences to bias MCTS, showing how self-acquired data can improve decision-making. Likewise, Perotto’s constructivist model CALM (2013) demonstrated an **anticipatory learning mechanism** that dynamically constructs its own state representations while interacting with a partially observable environment [@Perotto2013]. CALM could **model regularities at the sensorimotor level** and even form higher-level concepts, all within an intrinsically motivated agent–environment system【16†L199-L208】. These works illustrate the value of **adaptive tree-structured memories** (or forests of them) that grow as the agent encounters new situations. By organizing past sequences into a tree, an agent can share prefixes among many experiences and update the minimal necessary branches when a prediction fails. This ensures scalability: memory depth naturally adapts to the complexity of the environment, a design principle at the core of the Prediction Trees methodology.

### Unsupervised Structure Discovery in Sensorimotor Space

An exciting outcome of prediction-driven learning is the agent’s ability to **discover structure in its sensorimotor experience** without any labels or extrinsic goals. By interacting with the world and clustering predictable patterns, agents can unveil latent *regularities* corresponding to objects, spaces, or skills. Recent studies in developmental robotics and cognitive AI show that purely predictive learning can lead to emergent understanding of the environment’s structure. For instance, an agent with a moving camera that attempts to predict how its visual input changes can start to infer the layout of its surroundings – effectively learning **spatial structure by exploration**. Laflaquière and Ortiz demonstrated that a naive agent, by learning to predict its sensorimotor transitions, could recover the **topology and metric regularities of space** (e.g. the layout of a 2D environment) *with no prior knowledge or supervision* [@Laflaquiere2019].

In parallel, predictive learning can drive the emergence of object-like structures from raw interaction. Rather than relying on external labels, an agent can seek **invariant sensorimotor structures**—sets of sensations that remain self-consistent under the agent’s own actions—and use them as building blocks for object discovery. Le Hir, Sigaud, and Laflaquière showed that identifying such invariants is a **prerequisite for discovering objects**, demonstrating how unsupervised, prediction-based analysis of interaction data reveals stable components of the environment that behave like objects [@LeHir2018]. The key insight is that objects generate consistent sensorimotor contingencies (they move with the agent’s actions in self-consistent ways), which the agent can detect by seeking predictors that remain stable over time [@LeHir2018].

Such unsupervised discovery is closely linked to **intrinsic motivation**: the agent’s drive for prediction success leads it to partition the world into predictively distinct components (potentially aligning with human-interpretable structures like objects or places). These findings echo the hypothesis that **“learning to predict is enough”** to build a world model: an autonomous agent can bootstrap concepts like object permanence or spatial maps purely from its own interactive experience. Our work builds on this literature by using prediction as the sole learning signal to **foster autonomous structure discovery** [@ORegan2001], aiming to show that even a minimalist online memory of sensorimotor sequences can capture meaningful environmental regularities (e.g. recurring shapes or dynamics) without any task-specific reward or supervision. 